{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import configuration as config\n",
    "import mt_model_pointer as model\n",
    "from prepro import PreProcessing\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.set_random_seed(10)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['embeddings_dim'] = config.embeddings_dim\n",
    "params['lstm_cell_size'] = config.lstm_cell_size\n",
    "params['max_input_seq_length'] = config.max_input_seq_length\n",
    "params[\n",
    "    'max_output_seq_length'] = config.max_output_seq_length - 1  # inputs are all but last element, outputs are al but first element\n",
    "params['batch_size'] = config.batch_size\n",
    "params['pretrained_embeddings'] = config.use_pretrained_embeddings\n",
    "params['share_encoder_decoder_embeddings'] = config.share_encoder_decoder_embeddings\n",
    "params['use_pointer'] = config.use_pointer\n",
    "params['pretrained_embeddings_path'] = config.pretrained_embeddings_path\n",
    "params['pretrained_embeddings_are_trainable'] = config.pretrained_embeddings_are_trainable\n",
    "params['use_additional_info_from_pretrained_embeddings'] = config.use_additional_info_from_pretrained_embeddings\n",
    "params['max_vocab_size'] = config.max_vocab_size\n",
    "params['do_vocab_pruning'] = config.do_vocab_pruning\n",
    "params['use_reverse_encoder'] = config.use_reverse_encoder\n",
    "params['use_sentinel_loss'] = config.use_sentinel_loss\n",
    "params['lambd'] = config.lambd\n",
    "params['use_context_for_out'] = config.use_context_for_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = \"data/\"\n",
    "data = pickle.load(open(data_src + \"data.obj\",\"r\") )\n",
    "preprocessing = pickle.load(open(data_src + \"preprocessing.obj\",\"r\") )\n",
    "params['vocab_size'] = preprocessing.vocab_size\n",
    "params['preprocessing'] = preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained embeddings\n",
    "pretrained_embeddings = pickle.load(open(params['pretrained_embeddings_path'],\"r\"))\n",
    "word_to_idx = preprocessing.word_to_idx\n",
    "encoder_embedding_matrix = np.random.rand(params['vocab_size'], params['embeddings_dim'] )\n",
    "decoder_embedding_matrix = np.random.rand(params['vocab_size'], params['embeddings_dim'] )\n",
    "not_found_count = 0\n",
    "for token,idx in word_to_idx.items():\n",
    "    if token in pretrained_embeddings:\n",
    "        encoder_embedding_matrix[idx]=pretrained_embeddings[token]\n",
    "        decoder_embedding_matrix[idx]=pretrained_embeddings[token]\n",
    "params['encoder_embeddings_matrix'] = encoder_embedding_matrix\n",
    "params['decoder_embeddings_matrix'] = decoder_embedding_matrix\n",
    "\n",
    "if params['use_additional_info_from_pretrained_embeddings']:\n",
    "    additional_count=0\n",
    "    tmp=[]\n",
    "    for token in pretrained_embeddings:\n",
    "        if token not in preprocessing.word_to_idx:\n",
    "            preprocessing.word_to_idx[token] = preprocessing.word_to_idx_ctr\n",
    "            preprocessing.idx_to_word[preprocessing.word_to_idx_ctr] = token\n",
    "            preprocessing.word_to_idx_ctr+=1\n",
    "            tmp.append(pretrained_embeddings[token])\n",
    "            additional_count+=1\n",
    "    params['vocab_size'] = preprocessing.word_to_idx_ctr\n",
    "    tmp = np.array(tmp)\n",
    "    encoder_embedding_matrix = np.vstack([encoder_embedding_matrix,tmp])\n",
    "    decoder_embedding_matrix = np.vstack([decoder_embedding_matrix,tmp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained embedding size: 21583\n",
      "(192,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fawn', 4472),\n",
       " ('seagate', 21460),\n",
       " ('writings', 9207),\n",
       " ('glamis', 5325),\n",
       " ('hordes', 9208)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'pretrained embedding size:', len(pretrained_embeddings.keys())\n",
    "print pretrained_embeddings['figs'].shape\n",
    "word_to_idx.items()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    1   83    3   13 1009 4157   35  131    2]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1    7   79   18   12 3207    5    2]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1   97   13 9853   10   21  202    2]]\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\"Oh my, my bones ache so much\", \"I am in a rush .\", \"Give my compliments to your lady\"]\n",
    "\n",
    "pp = pickle.load(open(data_src + \"preprocessing.obj\",\"r\") )\n",
    "\n",
    "# the load data step\n",
    "input_texts = pp.preprocess(input_texts)\n",
    "sequences_input = []\n",
    "for i in input_texts:\n",
    "    tmp = [word_to_idx[pp.sent_start]]\n",
    "    for token in i:\n",
    "        if token not in word_to_idx:\n",
    "            tmp.append(word_to_idx[pp.unknown_word])\n",
    "        else:\n",
    "            tmp.append(word_to_idx[token])\n",
    "    tmp.append(word_to_idx[pp.sent_end])\n",
    "    sequences_input.append(tmp)\n",
    "\n",
    "sequences_input = pad_sequences(sequences_input, maxlen=config.max_input_seq_length,\n",
    "                               padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "sequences_input = np.array(sequences_input)\n",
    "print sequences_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from data/models/pointer_model6.ckpt\n",
      "INFO:tensorflow:Restoring parameters from data/models/pointer_model6.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Perform Greedy Inference\n",
    "inference_type = 'greedy'\n",
    "pointer_model = model.RNNModel(buckets_dict=None, mode='inference', params=params)\n",
    "# pointer_model.token_emb_mat = \n",
    "optimizer_type = \"adam\"\n",
    "lr = 0.001\n",
    "encoder_outputs = pointer_model.getEncoderModel(params,\n",
    "                                mode='inference', reuse=False)\n",
    "pointer_model.getDecoderModel(params,\n",
    "        encoder_outputs, is_training=False, mode='inference', reuse=False )\n",
    "\n",
    "# Bring saved model back\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "# TODO: define own saved model path\n",
    "saved_model_path = \"data/models/pointer_model6.ckpt\"\n",
    "saver.restore(sess, saved_model_path)\n",
    "\n",
    "\n",
    "encoder_outputs = pointer_model.getEncoderModel(params, mode='inference', reuse= True)\n",
    "decoder_outputs_inference, encoder_outputs, alpha_inference = \\\n",
    "    pointer_model.getDecoderModel(params, encoder_outputs, is_training=False, \n",
    "                                  mode='inference', reuse= True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder outputs [array([83, 81, 97, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83,\n",
      "       83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83]), array([ 4, 79, 13,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4]), array([  13,   12, 9853,   13,   13,   13,   13,   13,   13,   13,   13,\n",
      "         13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
      "         13,   13,   13,   13,   13,   13,   13,   13,   13,   13]), array([1009, 3207,   10, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009,\n",
      "       1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009,\n",
      "       1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009]), array([38,  5, 21, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38]), array([ 35,   2, 202,  35,  35,  35,  35,  35,  35,  35,  35,  35,  35,\n",
      "        35,  35,  35,  35,  35,  35,  35,  35,  35,  35,  35,  35,  35,\n",
      "        35,  35,  35,  35,  35,  35]), array([131,   2,   5, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131,\n",
      "       131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131,\n",
      "       131, 131, 131, 131, 131, 131]), array([30, 19,  2, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "       30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]), array([131,   5, 202, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131,\n",
      "       131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131,\n",
      "       131, 131, 131, 131, 131, 131]), array([30,  2,  5, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "       30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]), array([7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7]), array([ 58, 284,   2,  58,  58,  58,  58,  58,  58,  58,  58,  58,  58,\n",
      "        58,  58,  58,  58,  58,  58,  58,  58,  58,  58,  58,  58,  58,\n",
      "        58,  58,  58,  58,  58,  58]), array([  5,   5, 202,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "         5,   5,   5,   5,   5,   5]), array([2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2]), array([1009,    2,    2, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009,\n",
      "       1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009,\n",
      "       1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009]), array([   5, 3207,    2,    5,    5,    5,    5,    5,    5,    5,    5,\n",
      "          5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
      "          5,    5,    5,    5,    5,    5,    5,    5,    5,    5]), array([  2,   5, 202,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "         2,   2,   2,   2,   2,   2]), array([2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2]), array([1009,    2,    2, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009,\n",
      "       1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009,\n",
      "       1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009]), array([  5, 284,   2,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "         5,   5,   5,   5,   5,   5]), array([  2,   5, 284,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "         2,   2,   2,   2,   2,   2]), array([2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2]), array([1009,    2,    2, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009,\n",
      "       1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009,\n",
      "       1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009]), array([ 5, 19,  2,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5])]\n"
     ]
    }
   ],
   "source": [
    "# TODO: change batch size to custom ones from input\n",
    "batch_size = 32\n",
    "\n",
    "gap = batch_size - sequences_input.shape[0]\n",
    "for j in range(gap):\n",
    "    sequences_input = np.vstack((sequences_input, sequences_input[0]))\n",
    "                    \n",
    "        \n",
    "feed_dict = {\n",
    "    pointer_model.token_lookup_sequences_placeholder_inference: sequences_input\n",
    "}\n",
    "\n",
    "decoder_output, encoder_output, alpha_inf = np.array(\n",
    "    sess.run([decoder_outputs_inference,\n",
    "        encoder_outputs, alpha_inference], feed_dict=feed_dict))\n",
    "\n",
    "# vectorized encoder sentence\n",
    "# print 'encoder outputs', encoder_output\n",
    "\n",
    "# decoder output that we can reverse into sentences\n",
    "print 'decoder outputs', decoder_output\n",
    "\n",
    "# what exactly is alpha?\n",
    "# print 'alpha inference', alpha_inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', \"i'm\", 'give', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh', 'oh']\n",
      "[',', 'am', 'my', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
      "['my', 'a', 'compliments', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my']\n"
     ]
    }
   ],
   "source": [
    "reverse_dict = pp.idx_to_word\n",
    "output = \"\"\n",
    "for o in decoder_output[:3]:\n",
    "    print [reverse_dict[j] for j in o]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'sentstart', 'oh', 'unk', 'my', 'bones', 'ache', 'so', 'much', 'sentend']\n",
      "['padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'sentstart', 'i', 'am', 'in', 'a', 'rush', '.', 'sentend']\n",
      "['padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'padword', 'sentstart', 'give', 'my', 'compliments', 'to', 'your', 'lady', 'sentend']\n"
     ]
    }
   ],
   "source": [
    "reverse_dict = pp.idx_to_word\n",
    "output = \"\"\n",
    "for o in sequences_input[:3]:\n",
    "    print [reverse_dict[j] for j in o]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
